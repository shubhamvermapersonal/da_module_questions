{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3rKVwXwmq4VCIEdyyzyXE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhamvermapersonal/da_module_questions/blob/main/Machine_Learning_1_Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theory Questions**"
      ],
      "metadata": {
        "id": "k6Mjg4GZ4huh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **What is a Parameter?**\n",
        "In the context of machine learning, a **parameter** is a value that helps define a model's behavior. For example, in linear regression, the coefficients of the variables (like the slope and intercept) are parameters. Parameters are learned from the training data and define the relationship between input features and the output. These parameters are crucial for making predictions once the model is trained.\n",
        "\n",
        "### 2. **What is Correlation? What Does Negative Correlation Mean?**\n",
        "**Correlation** refers to the statistical relationship between two variables. It measures how much one variable changes when another variable changes. The correlation value ranges from -1 to 1:\n",
        "- A **positive correlation** (near 1) means that as one variable increases, the other increases as well.\n",
        "- A **negative correlation** (near -1) means that as one variable increases, the other decreases.\n",
        "For example, the relationship between the temperature and ice cream sales might be positively correlated (higher temperatures lead to higher sales). On the other hand, the relationship between the number of umbrellas sold and the amount of sunshine could be negatively correlated.\n",
        "\n",
        "### 3. **Define Machine Learning. What Are the Main Components in Machine Learning?**\n",
        "**Machine Learning** (ML) is a subset of artificial intelligence that involves training algorithms to learn from and make predictions or decisions based on data, without being explicitly programmed for every situation. The main components in machine learning include:\n",
        "- **Data**: The raw information that the model will use to learn.\n",
        "- **Algorithm**: The method used to learn from the data (e.g., decision trees, neural networks, etc.).\n",
        "- **Model**: The result of training the algorithm on data; it is used to make predictions.\n",
        "- **Training Process**: The phase where the model learns patterns from the data.\n",
        "- **Evaluation**: Assessing how well the model performs using metrics like accuracy or loss.\n",
        "\n",
        "### 4. **How Does Loss Value Help in Determining Whether the Model Is Good or Not?**\n",
        "The **loss value** (or cost function) measures how well the model's predictions match the actual outcomes. A **low loss** means that the model's predictions are close to the actual values, while a **high loss** indicates that the model is performing poorly. In machine learning, the goal is to minimize the loss function during the training process. Common loss functions include mean squared error (MSE) for regression tasks and cross-entropy loss for classification tasks.\n",
        "\n",
        "### 5. **What Are Continuous and Categorical Variables?**\n",
        "- **Continuous variables** are numeric variables that can take any value within a given range (e.g., height, weight, temperature). They represent measurable quantities and are often used in regression tasks.\n",
        "- **Categorical variables** represent categories or groups and can take values that are labels (e.g., gender, color, city). They are usually handled in classification tasks.\n",
        "\n",
        "### 6. **How Do We Handle Categorical Variables in Machine Learning? What Are the Common Techniques?**\n",
        "Categorical variables need to be transformed into a format that can be understood by machine learning algorithms (which usually expect numerical input). Some common techniques for handling categorical variables include:\n",
        "- **One-Hot Encoding**: Converts each category into a binary vector. For example, a \"color\" variable with values \"red,\" \"green,\" and \"blue\" would be transformed into three columns: one for red, one for green, and one for blue.\n",
        "- **Label Encoding**: Assigns an integer to each category. For example, \"red\" might become 0, \"green\" 1, and \"blue\" 2.\n",
        "- **Ordinal Encoding**: For ordinal variables (variables that have a specific order, like \"small,\" \"medium,\" \"large\"), this method assigns integers based on the order.\n",
        "\n",
        "### 7. **What Do You Mean by Training and Testing a Dataset?**\n",
        "- **Training a dataset** involves using a subset of the available data to train the machine learning model. This data helps the model learn patterns and relationships.\n",
        "- **Testing a dataset** involves using a separate subset of the data (that the model has not seen during training) to evaluate the model’s performance. The test set is crucial to determine how well the model generalizes to new, unseen data.\n",
        "\n",
        "### 8. **What is sklearn.preprocessing?**\n",
        "**`sklearn.preprocessing`** is a module from the `scikit-learn` library in Python that provides functions for transforming and scaling data. Some of the common preprocessing techniques include:\n",
        "- **Standardization**: Scaling the data so that it has a mean of 0 and a standard deviation of 1.\n",
        "- **Normalization**: Rescaling the data to a specific range, usually [0, 1].\n",
        "- **Imputation**: Filling missing values with a specified value or method.\n",
        "- **Encoding**: Converting categorical data into numerical form using techniques like one-hot encoding.\n",
        "\n",
        "### 9. **What is a Test Set?**\n",
        "A **test set** is a portion of the dataset that is not used during the model training phase. It is reserved for evaluating how well the model performs on unseen data. Using a test set helps in assessing the generalization ability of the model. It’s important that the test set is not used in any part of the training process to avoid overfitting.\n",
        "\n",
        "### 10. **How Do We Split Data for Model Fitting (Training and Testing) in Python? How Do You Approach a Machine Learning Problem?**\n",
        "To split data for model fitting in Python, the `train_test_split` function from the `sklearn.model_selection` module is commonly used:\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "This splits the data into training data (80%) and testing data (20%), with `X` being the features and `y` being the target variable.\n",
        "\n",
        "**Approaching a Machine Learning Problem**:\n",
        "1. **Understand the Problem**: Define the problem clearly (e.g., classification, regression).\n",
        "2. **Collect and Prepare Data**: Gather relevant data and preprocess it by handling missing values, scaling features, and encoding categorical variables.\n",
        "3. **Choose a Model**: Select an appropriate machine learning model (e.g., decision tree, random forest, neural network).\n",
        "4. **Train the Model**: Use the training dataset to train the model and tune hyperparameters.\n",
        "5. **Evaluate the Model**: Test the model using the test set and assess its performance using evaluation metrics (accuracy, precision, recall, etc.).\n",
        "6. **Iterate and Improve**: If necessary, refine the model by adjusting parameters, adding features, or trying different models.\n",
        "\n",
        "### 11. **Why Do We Have to Perform EDA Before Fitting a Model to the Data?**\n",
        "**Exploratory Data Analysis (EDA)** is essential before fitting a model because it helps you understand the data’s structure, patterns, and potential issues. EDA allows you to:\n",
        "- **Identify missing values**: These may need to be handled before training a model.\n",
        "- **Visualize distributions**: It helps in checking if features are skewed or follow a normal distribution, which might affect certain models.\n",
        "- **Detect outliers**: Outliers can skew model performance and might need to be removed or transformed.\n",
        "- **Understand relationships**: EDA helps identify correlations and relationships between features, aiding in selecting relevant features for modeling.\n",
        "\n",
        "### 12. **What is Correlation?**\n",
        "**Correlation** measures the statistical relationship between two or more variables. It shows the strength and direction of the relationship:\n",
        "- A **positive correlation** means that as one variable increases, the other also tends to increase.\n",
        "- A **negative correlation** means that as one variable increases, the other tends to decrease.\n",
        "- A **correlation of 0** means no linear relationship exists between the variables.\n",
        "\n",
        "### 13. **What Does Negative Correlation Mean?**\n",
        "A **negative correlation** between two variables means that as one variable increases, the other decreases. For example, there’s a negative correlation between the amount of time spent studying and the amount of errors made in an exam. As study time increases, errors decrease.\n",
        "\n",
        "### 14. **How Can You Find Correlation Between Variables in Python?**\n",
        "To find the correlation between variables in Python, you can use the `pandas` library. The `corr()` method calculates pairwise correlation between columns in a DataFrame:\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    'x': [1, 2, 3, 4, 5],\n",
        "    'y': [5, 4, 3, 2, 1]\n",
        "})\n",
        "correlation = df.corr()  # Correlation matrix\n",
        "print(correlation)\n",
        "```\n",
        "This will return the correlation between `x` and `y`.\n",
        "\n",
        "### 15. **What is Causation? Explain the Difference Between Correlation and Causation with an Example.**\n",
        "**Causation** means that one variable directly causes changes in another variable. Unlike correlation, which merely indicates a relationship, causation implies a cause-and-effect scenario.\n",
        "- **Example of correlation**: There might be a correlation between ice cream sales and temperature, but buying ice cream does not *cause* the temperature to rise. It’s simply that both tend to increase as summer arrives.\n",
        "- **Example of causation**: Smoking causes lung cancer. If we observe a relationship, it’s not just a pattern but a cause-and-effect link.\n",
        "\n",
        "### 16. **What is an Optimizer? What Are Different Types of Optimizers? Explain Each with an Example.**\n",
        "An **optimizer** is an algorithm that adjusts the model’s weights to minimize the loss function during training. The objective is to improve model accuracy.\n",
        "Common types of optimizers:\n",
        "- **Gradient Descent**: A simple optimization technique where the model’s weights are updated based on the gradient (slope) of the loss function.\n",
        "  - Example: In linear regression, gradient descent can be used to minimize the error between the predicted and actual values by adjusting the weights.\n",
        "- **Stochastic Gradient Descent (SGD)**: A variation of gradient descent that updates weights using only a subset of data (a batch), making it faster for large datasets.\n",
        "- **Adam Optimizer**: Combines the benefits of momentum and adaptive learning rates. It’s widely used in deep learning for efficient training.\n",
        "  - Example: In a neural network, Adam helps improve the learning rate during training, speeding up convergence.\n",
        "- **RMSProp**: An adaptive learning rate optimizer that divides the gradient by a moving average of squared gradients to handle large gradients.\n",
        "\n",
        "### 17. **What is sklearn.linear_model?**\n",
        "**`sklearn.linear_model`** is a module in the `scikit-learn` library that provides linear models for regression and classification tasks. It includes:\n",
        "- **LinearRegression**: For linear regression problems.\n",
        "- **LogisticRegression**: For binary classification tasks.\n",
        "- **Ridge** and **Lasso**: Linear regression models with L2 and L1 regularization, respectively, to prevent overfitting.\n",
        "\n",
        "### 18. **What Does model.fit() Do? What Arguments Must Be Given?**\n",
        "**`model.fit()`** is used to train the model on the training dataset. It takes at least two arguments:\n",
        "- `X_train`: The input data (features) used to train the model.\n",
        "- `y_train`: The target data (labels) corresponding to `X_train`.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "The model learns from the data, adjusting its parameters to minimize the error.\n",
        "\n",
        "### 19. **What Does model.predict() Do? What Arguments Must Be Given?**\n",
        "**`model.predict()`** is used to make predictions after the model has been trained. It requires the input data (`X_test`) as an argument:\n",
        "```python\n",
        "predictions = model.predict(X_test)\n",
        "```\n",
        "Here, `X_test` contains the features of the test dataset, and the model predicts the target values based on the learned patterns.\n",
        "\n",
        "### 20. **What Are Continuous and Categorical Variables?**\n",
        "- **Continuous variables** are numeric and can take any value within a range (e.g., height, weight, temperature).\n",
        "- **Categorical variables** are discrete and represent categories or groups (e.g., gender, country, product type).\n",
        "\n",
        "### 21. **What is Feature Scaling? How Does It Help in Machine Learning?**\n",
        "**Feature scaling** refers to the process of standardizing or normalizing the range of independent variables or features in a dataset. Some machine learning algorithms are sensitive to the scale of data (e.g., K-Nearest Neighbors, SVM, gradient descent-based models). Feature scaling ensures that each feature contributes equally to the model's performance.\n",
        "- **Standardization**: Centers the data around zero by subtracting the mean and dividing by the standard deviation.\n",
        "- **Normalization**: Scales the data to a specific range (usually [0, 1]).\n",
        "\n",
        "### 22. **How Do We Perform Scaling in Python?**\n",
        "In Python, you can use `sklearn.preprocessing` to scale data:\n",
        "- **Standardization** using `StandardScaler`:\n",
        "  ```python\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler = StandardScaler()\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "  ```\n",
        "- **Normalization** using `MinMaxScaler`:\n",
        "  ```python\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "  X_normalized = scaler.fit_transform(X)\n",
        "  ```\n",
        "\n",
        "### 23. **What is sklearn.preprocessing?**\n",
        "**`sklearn.preprocessing`** is a module in `scikit-learn` that provides tools to preprocess and transform data. Some common functions include:\n",
        "- **StandardScaler**: For standardization (z-score normalization).\n",
        "- **MinMaxScaler**: For scaling features to a given range.\n",
        "- **OneHotEncoder**: For encoding categorical variables into binary vectors.\n",
        "- **LabelEncoder**: For encoding labels as integers.\n",
        "\n",
        "### 24. **How Do We Split Data for Model Fitting (Training and Testing) in Python?**\n",
        "You can use the `train_test_split` function from `sklearn.model_selection` to split your data into training and testing sets:\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "This splits the data, with 80% used for training and 20% for testing.\n",
        "\n",
        "### 25. **Explain Data Encoding?**\n",
        "**Data encoding** is the process of converting categorical variables into a format that can be understood by machine learning algorithms. Common encoding methods include:\n",
        "- **One-Hot Encoding**: Converts each category into a binary vector. For example, if a column has categories `['red', 'green', 'blue']`, it would be converted to separate columns for each color with 1s and 0s indicating the presence of each category.\n",
        "- **Label Encoding**: Assigns a unique integer to each category. For instance, `['red', 'green', 'blue']` might become `[0, 1, 2]`.\n"
      ],
      "metadata": {
        "id": "pzvi7Vjj4mWL"
      }
    }
  ]
}